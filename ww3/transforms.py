from abc import abstractmethod
from functools import cached_property
from pathlib import Path

import numpy as np
import torch
from mfai.pytorch.namedtensor import NamedTensor
from torch import Tensor, nn
from ww3.settings import SCRATCH_PATH, STATS_PATH


class ReversibleTransformMixin:
    """Mixin class that enables a transform to specify a reverse operation."""

    @cached_property
    @abstractmethod
    def _reverse_op(self) -> nn.Module:
        raise NotImplementedError()

    def undo(self, x: NamedTensor, y: NamedTensor) -> tuple[NamedTensor, NamedTensor]:
        """The transform's undo operation."""
        return self._reverse_op(x, y)


class NaNToNum(nn.Module, ReversibleTransformMixin):
    """Replace NaNs by a number in input and target."""

    def __init__(self, num: float = 0.0, downscaling_stride: int = 25) -> None:
        super().__init__()
        self.num = num
        self.downscaling_stride = downscaling_stride

    @cached_property
    def _reverse_op(self) -> "ReverseNaNToNum":
        return ReverseNaNToNum(self.downscaling_stride)

    def forward(
        self, x: NamedTensor, y: NamedTensor
    ) -> tuple[NamedTensor, NamedTensor]:
        """
        Args:
            x: input data
            y: target data

        Returns:
            NamedTensor: input data without NaN.
            NamedTensor: target data without NaN.
        """
        new_x_tensor = torch.nan_to_num(x.tensor, nan=self.num)
        new_y_tensor = torch.nan_to_num(y.tensor, nan=self.num)
        new_x = NamedTensor.new_like(new_x_tensor, x)
        new_y = NamedTensor.new_like(new_y_tensor, y)
        return new_x, new_y


class ReverseNaNToNum(nn.Module):
    """Put back NaNs outside of sea mask."""

    def __init__(self, downscaling_stride: int = 25) -> None:
        super().__init__()
        mask_mfwam = np.load(SCRATCH_PATH / "mfwam_bathymetry.npz")["arr_0"]
        mask_mfwam = np.expand_dims(mask_mfwam, 0)
        self.mask_mfwam = mask_mfwam[:, ::downscaling_stride, ::downscaling_stride]

    def put_nans(self, nt: NamedTensor, mask: Tensor) -> NamedTensor:
        masked_features: list[Tensor] = []
        for feature_name in nt.feature_names:
            if any([name in feature_name for name in ["mask", "bathy", "arpege"]]):
                masked_features.append(nt[feature_name])
            else:
                masked_tensor = torch.where(mask > 0, nt[feature_name], float("nan"))
                masked_features.append(masked_tensor)

        # Build masked feature tensor
        masked_features_tensor = torch.cat(
            tensors=masked_features, dim=nt.feature_dim_idx
        )
        # Recreate a NamedTensor with the masked features
        return NamedTensor.new_like(tensor=masked_features_tensor, other=nt)

    def forward(
        self, x: NamedTensor, y: NamedTensor
    ) -> tuple[NamedTensor, NamedTensor]:
        mask_ww3 = x["landsea_mask"]
        mask_mfwam = Tensor(self.mask_mfwam).to(x.tensor.device)
        return self.put_nans(x, mask_mfwam), self.put_nans(y, mask_ww3)


def load_stats(path: str | Path) -> dict[str, dict[str, float]]:
    """Loads a dictionary of statistics on the training data."""
    if isinstance(path, str):
        path = Path(path)
    if not path.exists():
        raise FileNotFoundError(
            "Statistics file not found. Please run `python bin/compute_stats.py`"
        )
    return torch.load(path)


class Normalize(nn.Module, ReversibleTransformMixin):
    """Normalizes data.
    Requires a pre processed stats file generated by `python bin/compute_stats.py`
    """

    @cached_property
    def _reverse_op(self) -> "ReverseNormalize":
        return ReverseNormalize(self.stats_file_path, self.interval)

    def __init__(
        self,
        stats_file_path: Path | str = STATS_PATH,
        interval: list[int] = [0, 1],
    ) -> None:
        super().__init__()

        self.stats_file_path = stats_file_path
        self.stats_dict = load_stats(self.stats_file_path)
        self.interval = interval

    def normalize_namedtensor(self, nt: NamedTensor) -> NamedTensor:
        # Normalize each feature
        normalized_features: list[Tensor] = []
        for feature_name in nt.feature_names:
            mini = self.stats_dict[feature_name]["min"]
            maxi = self.stats_dict[feature_name]["max"]
            # Normalize between 0 and 1
            normalized_feature = (nt[feature_name] - mini) / (maxi - mini)
            # Normalize in the interval
            normalized_feature = (
                normalized_feature * (self.interval[1] - self.interval[0])
                + self.interval[0]
            )
            normalized_features.append(normalized_feature)

        # Build normalized feature tensor
        normalized_features_tensor = torch.cat(
            tensors=normalized_features, dim=nt.feature_dim_idx
        )
        # Recreate a NamedTensor with the normalized features
        return NamedTensor.new_like(tensor=normalized_features_tensor, other=nt)

    def forward(
        self, x: NamedTensor, y: NamedTensor
    ) -> tuple[NamedTensor, NamedTensor]:
        return self.normalize_namedtensor(x), self.normalize_namedtensor(y)


class ReverseNormalize(nn.Module):
    """Inverse normalization of data."""

    def __init__(
        self,
        stats_file_path: Path | str = STATS_PATH,
        interval: list[int] = [0, 1],
    ) -> None:
        super().__init__()
        self.stats_file_path = stats_file_path
        self.stats_dict = load_stats(self.stats_file_path)
        self.interval = interval

    def denormalize_namedtensor(self, nt: NamedTensor) -> NamedTensor:
        denormalized_features: list[Tensor] = []
        for feature_name in nt.feature_names:
            mini = self.stats_dict[feature_name]["min"]
            maxi = self.stats_dict[feature_name]["max"]
            # Normalize between 0 and 1
            normalized_feature = (nt[feature_name] - self.interval[0]) / (
                self.interval[1] - self.interval[0]
            )
            denormalized_features.append(normalized_feature * (maxi - mini) + mini)

        denormalized_features_tensor = torch.cat(
            tensors=denormalized_features, dim=nt.feature_dim_idx
        )
        return NamedTensor.new_like(tensor=denormalized_features_tensor, other=nt)

    def forward(
        self, x: NamedTensor, y: NamedTensor
    ) -> tuple[NamedTensor, NamedTensor]:
        return self.denormalize_namedtensor(x), self.denormalize_namedtensor(y)
